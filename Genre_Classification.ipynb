{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies, load spaCy's core model (small)\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cleaned csv into dataframe\n",
    "df = pd.read_csv(\"Resources/wiki_movie_plots_CLEANED.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spaCy to process each movie plot string.\n",
    "nostopwords = []\n",
    "for i in range (len(df)):\n",
    "    doc = nlp(df.Plot[i])\n",
    "    \n",
    "    # Tokenize strings - use full text (verbatim, and lowercased) for each token; remove common stop words and punctuation\n",
    "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    nostopwords.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for token lists\n",
    "df['no_stop_words'] = nostopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df with tokens to new csv\n",
    "df.to_csv(\"Resources/plots_tokenized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df into 80:20 train:test split\n",
    "train, test = train_test_split(df, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy function to pass into 'tokenizer' parameter instead of an actual tokenizing function,\n",
    "# since we already have our list of lists of tokens (nostopwords)\n",
    "def do_nothing(tokens):\n",
    "    return tokens\n",
    "\n",
    "# Use TfidfVectorizer (equivalent to CountVectorizer then Tfidftransformer)\n",
    "# Generates Bag of Words matrices of token counts (vectors). Use ngrams of size n=1\n",
    "# Then transforms count matrices to normalized tf-idf representations\n",
    "\n",
    "vectorizer = TfidfVectorizer(input='content', tokenizer=do_nothing, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear dimensionality reduction using truncated singular value decomposition (SVD)\n",
    "# Contrary to principle component analysis (PCA), this estimator does not center the data \n",
    "# before computing the SVD. This means it can work with scipy.sparse matrices efficiently\n",
    "svd = TruncatedSVD(n_components=200, n_iter=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug step that prints shape. Does not actually fit to or transform data.\n",
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(X.shape)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Linear SVC support vector machine as our classifier\n",
    "clf = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spaCy pipeline for processing data. 1: create count vectors; 2: classify!\n",
    "pipe = Pipeline([('vectorizer', vectorizer), \n",
    "                 ('debug', Debug()),\n",
    "                 ('svd', svd),\n",
    "                 ('clf', clf)\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text to be classified = 'Plot', labels = 'Genre'; assign for both training and testing data\n",
    "train1 = train['Plot'].tolist()\n",
    "labels_train1 = train['Genre'].tolist()\n",
    "\n",
    "test1 = test['Plot'].tolist()\n",
    "labels_test1 = test['Genre'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21524, 594)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit our pipeline/model to the training data\n",
    "pipe.fit(train1, labels_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5381, 594)\n",
      "accuracy: 0.29920089202750416\n"
     ]
    }
   ],
   "source": [
    "# No need to transform test data - pipeline takes care of that\n",
    "preds = pipe.predict(test1)\n",
    "print(\"accuracy:\", accuracy_score(labels_test1, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 594)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['animation'], dtype='<U15')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random forest\n",
    "extra trees\n",
    "gradient booster\n",
    "\n",
    "matt has better + truncated svd\n",
    "experiment with n_components 120-200, maybe more?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
