{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies, load spaCy's core model (small)\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cleaned csv into dataframe\n",
    "df = pd.read_csv(\"Resources/wiki_movie_plots_CLEANED.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spaCy to process each movie plot string.\n",
    "nostopwords = []\n",
    "for i in range (len(df)):\n",
    "    doc = nlp(df.Plot[i])\n",
    "    \n",
    "    # Tokenize strings - use full text (verbatim, and lowercased) for each token; remove common stop words and punctuation\n",
    "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    nostopwords.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for token lists\n",
    "df['no_stop_words'] = nostopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df with tokens to new csv\n",
    "df.to_csv(\"Resources/plots_tokenized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df into 80:20 train:test split\n",
    "train, test = train_test_split(df, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text to be classified = 'Plot', labels = 'Genre'; assign for both training and testing data\n",
    "train1 = train['Plot'].tolist()\n",
    "train1_labels = train['Genre'].tolist()\n",
    "\n",
    "test1 = test['Plot'].tolist()\n",
    "test1_labels = test['Genre'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy function to pass into 'tokenizer' parameter instead of an actual tokenizing function,\n",
    "# since we already have our list of lists of tokens (nostopwords)\n",
    "def do_nothing(tokens):\n",
    "    return tokens\n",
    "\n",
    "# Use TfidfVectorizer (equivalent to CountVectorizer then Tfidftransformer)\n",
    "# Generates Bag of Words matrices of token counts (vectors). Use ngrams of size n=1\n",
    "# Then transforms count matrices to normalized tf-idf representations\n",
    "vectorizer = TfidfVectorizer(input='content', tokenizer=do_nothing, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear dimensionality reduction using truncated singular value decomposition (SVD)\n",
    "# In the context of working on term count/tf-idf matrices as returned vectorizers - AKA latent semantic analysis (LSA)\n",
    "# Works with scipy.sparse matrices efficiently\n",
    "tSVD = TruncatedSVD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug step that simply prints shape of data. Does not actually fit to or transform data.\n",
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(X.shape)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Extra Trees (EXTreme RAndom forests) ensemble estimator as our classifier. \n",
    "# Should perform better than Random Forest in presence of noisy features. \n",
    "# Lower variance = lower variability of model prediction for a given data point = generalizes better on test data\n",
    "classifier = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spaCy pipeline for processing data. \n",
    "# 1: create tf-idf vectors; 2: dimensionality reduction; 3: classify using Extra Trees\n",
    "pipe = Pipeline([('vectorizer', vectorizer), \n",
    "#                 ('debug', Debug()),\n",
    "                 ('tSVD', tSVD),\n",
    "#                ('debug2', Debug()),\n",
    "                 ('ETclassifier', classifier)\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build hyperparameter grid for RandomizedSearchCV\n",
    "# Randomly pick combinations of parameters to try for Truncated SVD and Extra Trees Classifier\n",
    "\n",
    "param_grid = {\n",
    "    # n_components = desired dimensionality of output data\n",
    "    # n_iter = number of iterations for randomized SVD solver. default = 5\n",
    "    'tSVD__n_components': [50, 150, 300, 500],\n",
    "    'tSVD__n_iter': [5, 7, 10],\n",
    "    \n",
    "    # n_estimators = number of trees in the forest\n",
    "    # max_features = max number of features considered for splitting a node\n",
    "    # max_depth = max number of levels in each decision tree\n",
    "    # min_samples_split = min number of data points placed in a node before the node is split\n",
    "    # min_samples_leaf = min number of data points allowed in a leaf node\n",
    "    # bootstrap = method for sampling data points (with or without replacement)\n",
    "    'ETclassifier__n_estimators': [10, 100, 200, 500, 1000],\n",
    "    'ETclassifier__max_depth': [2, 4, 6, 8, 10, None], \n",
    "    'ETclassifier__min_samples_split': [2, 5, 10],\n",
    "    'ETclassifier__min_samples_leaf': [1, 2, 4],\n",
    "    'ETclassifier__max_features': ['sqrt', 'log2'],\n",
    "    'ETclassifier__bootstrap': [True, False],\n",
    "}\n",
    "\n",
    "# Number of combinations to sample = 50 (more increases runtime)\n",
    "# cv = 3 (3-fold cross-validation)\n",
    "# n_jobs = -1 (uses all available cores/processors)\n",
    "search = RandomizedSearchCV(estimator=pipe, \n",
    "                            param_distributions=param_grid, \n",
    "                            n_iter=50, \n",
    "                            n_jobs=-1, \n",
    "                            cv=3,\n",
    "                            verbose=3, \n",
    "                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    }
   ],
   "source": [
    "# Run randomizedsearchCV, try different param combos to fit to the training data\n",
    "search.fit(train1, train1_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best performing combination of parameters\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5381, 594)\n",
      "(5381, 100)\n",
      "accuracy: 0.297342501393793\n"
     ]
    }
   ],
   "source": [
    "# (No need to transform test data - pipeline takes care of that)\n",
    "# Print classification accuracy score\n",
    "preds = pipe.predict(test1)\n",
    "print(\"accuracy:\", accuracy_score(test1_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pickle to save best estimator from search to a .sav file\n",
    "filename = 'movie_genre_classifier.sav'\n",
    "pickle.dump(search.best_estimator_, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_predict = load_model.predict([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.predict([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
