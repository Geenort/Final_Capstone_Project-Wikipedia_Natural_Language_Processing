{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies, load spaCy's core model (small)\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cleaned csv into dataframe\n",
    "df = pd.read_csv(\"Resources/wiki_movie_plots_CLEANED.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spaCy to process each movie plot string.\n",
    "nostopwords = []\n",
    "for i in range (len(df)):\n",
    "    doc = nlp(df.Plot[i])\n",
    "    \n",
    "    # Tokenize strings - use full text (verbatim, and lowercased) for each token\n",
    "    # remove common stop words and punctuation\n",
    "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    nostopwords.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for token lists\n",
    "df['no_stop_words'] = nostopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df with tokens to new csv\n",
    "df.to_csv(\"Resources/plots_tokenized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df into 80:20 train:test split\n",
    "train, test = train_test_split(df, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text to be classified = 'Plot', labels = 'Genre'; assign for both training and testing data\n",
    "train1 = train['Plot'].tolist()\n",
    "train1_labels = train['Genre'].tolist()\n",
    "\n",
    "test1 = test['Plot'].tolist()\n",
    "test1_labels = test['Genre'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy function to pass into 'tokenizer' parameter instead of an actual tokenizing function,\n",
    "# since we already have our list of lists of tokens (nostopwords)\n",
    "def do_nothing(tokens):\n",
    "    return tokens\n",
    "\n",
    "# Use TfidfVectorizer (equivalent to CountVectorizer then Tfidftransformer)\n",
    "# Generates Bag of Words matrices of token counts (vectors). Use ngrams of size n=1\n",
    "# Then transforms count matrices to normalized tf-idf representations\n",
    "vectorizer = TfidfVectorizer(input='content', tokenizer=do_nothing, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear dimensionality reduction using truncated singular value decomposition (SVD)\n",
    "# In the context of working on term count/tf-idf matrices as returned vectorizers, AKA latent semantic analysis (LSA)\n",
    "# Works with scipy.sparse matrices efficiently\n",
    "tSVD = TruncatedSVD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug step that simply prints shape of data. Does not actually fit to or transform data.\n",
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(X.shape)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Extra Trees (EXTreme RAndom forests) ensemble estimator as our classifier. \n",
    "# Should perform better than Random Forest in presence of noisy features. \n",
    "# Lower variance = lower variability of model prediction for a given data point = generalizes better on test data\n",
    "classifier = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spaCy pipeline for processing data. \n",
    "# 1: create tf-idf vectors; 2: dimensionality reduction; 3: classify using Extra Trees\n",
    "pipe = Pipeline([('vectorizer', vectorizer), \n",
    "                 ('debug', Debug()),\n",
    "                 ('tSVD', tSVD),\n",
    "                 ('debug2', Debug()),\n",
    "                 ('ETclassifier', classifier)\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build hyperparameter grid for RandomizedSearchCV\n",
    "# Randomly pick combinations of parameters to try for Truncated SVD and Extra Trees Classifier\n",
    "\n",
    "param_grid = {\n",
    "    # n_components = desired dimensionality of output data\n",
    "    # n_iter = number of iterations for randomized SVD solver. default = 5\n",
    "    'tSVD__n_components': [50, 150, 250, 400],\n",
    "    'tSVD__n_iter': [5, 7, 10],\n",
    "    \n",
    "    # n_estimators = number of trees in the forest\n",
    "    # max_features = max number of features considered for splitting a node\n",
    "    # max_depth = max number of levels in each decision tree\n",
    "    # min_samples_split = min number of data points placed in a node before the node is split\n",
    "    # min_samples_leaf = min number of data points allowed in a leaf node\n",
    "    # bootstrap = method for sampling data points (with or without replacement)\n",
    "    'ETclassifier__n_estimators': [10, 100, 200, 500, 1000],\n",
    "    'ETclassifier__max_depth': [2, 4, 6, 8, 10, None], \n",
    "    'ETclassifier__min_samples_split': [2, 5, 10],\n",
    "    'ETclassifier__min_samples_leaf': [1, 2, 4],\n",
    "    'ETclassifier__max_features': ['sqrt', 'log2'],\n",
    "    'ETclassifier__bootstrap': [True, False],\n",
    "}\n",
    "\n",
    "# Number of combinations to sample = 50 (more increases runtime)\n",
    "# cv = 3 (3-fold cross-validation)\n",
    "# n_jobs = -1 (uses all available cores/processors)\n",
    "search = RandomizedSearchCV(estimator=pipe, \n",
    "                            param_distributions=param_grid, \n",
    "                            n_iter=50, \n",
    "                            n_jobs=-1, \n",
    "                            cv=3,\n",
    "                            verbose=3, \n",
    "                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed: 21.3min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 27.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...s='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False))]),\n",
       "          fit_params=None, iid='warn', n_iter=50, n_jobs=-1,\n",
       "          param_distributions={'tSVD__n_components': [50, 150, 250, 400], 'tSVD__n_iter': [5, 7, 10], 'ETclassifier__n_estimators': [10, 100, 200, 500, 1000], 'ETclassifier__max_depth': [2, 4, 6, 8, 10, None], 'ETclassifier__min_samples_split': [2, 5, 10], 'ETclassifier__min_samples_leaf': [1, 2, 4], 'ETclassifier__max_features': ['sqrt', 'log2'], 'ETclassifier__bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run randomizedsearchCV, try different param combos to fit to the training data\n",
    "search.fit(train1, train1_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tSVD__n_iter': 5,\n",
       " 'tSVD__n_components': 50,\n",
       " 'ETclassifier__n_estimators': 200,\n",
       " 'ETclassifier__min_samples_split': 5,\n",
       " 'ETclassifier__min_samples_leaf': 1,\n",
       " 'ETclassifier__max_features': 'sqrt',\n",
       " 'ETclassifier__max_depth': None,\n",
       " 'ETclassifier__bootstrap': False}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print best performing combination of parameters\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.3071919717524624\n"
     ]
    }
   ],
   "source": [
    "# (No need to transform test data - pipeline takes care of that)\n",
    "# Print classification accuracy score\n",
    "preds = search.predict(test1)\n",
    "print(\"accuracy:\", accuracy_score(test1_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pickle to save best estimator from search to a .sav file\n",
    "filename = 'movie_genre_classifier.sav'\n",
    "pickle.dump(search.best_estimator_, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pickle to load model\n",
    "filename = 'movie_genre_classifier.sav'\n",
    "model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict genre from input string. Input must be a list\n",
    "Plot_to_Predict = ''\n",
    "prediction = model.predict([Plot_to_Predict.lower()])\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
